[project]
name = "semantic-kernel-workshop"
version = "0.1.0"
description = "A workshop for Semantic Kernel"
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "a2a-sdk>=0.2.8",
    "fastapi>=0.116.0",
    "fastmcp>=2.2.0",
    "flask>=3.1.1",
    "flask-cors>=6.0.1",
    "httpx>=0.28.1",
    "jupyter>=1.1.1",
    "jupyterlab>=4.5.0a0",
    "mermaid-py>=0.7.1",
    "python-a2a>=0.5.9",
    "python-dotenv==1.0.1",
    "requests>=2.32.3",
    "rich>=14.0.0",
    "semantic-kernel==1.31.0",
    "uvicorn>=0.35.0",
    "einops>=0.8.1",
    "flash-attn",
    "huggingface-hub>=0.34.4",
    "hydra-core>=1.3.2",
    "ninja>=1.11.1.4",
    "omegaconf>=2.3.0",
    "packaging>=24.1",
    "pydantic>=2.11.7",
    "setuptools>=70.2.0",
    "setuptools-scm>=8.3.1",
    "torch<2.8.0",
    "torchaudio<2.8.0",
    "torchvision<0.23.0",
    "tqdm>=4.66.5",
    "wandb>=0.21.1",
    "wheel>=0.45.1",
    "transformers>=4.55.2",
    "bitsandbytes>=0.47.0",
    "accelerate>=1.10.0",
    "sentence-transformers>=5.1.0",
]

[tool.uv]
no-build-isolation-package = ["flash-attn"]

[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[project.optional-dependencies]
build = ["torch", "setuptools", "packaging"]
compile = ["flash-attn"]

[[tool.uv.dependency-metadata]]
name = "flash-attn"
version = "2.8.2"
requires-dist = ["torch", "einops"]

[tool.uv.sources]
flash-attn = { url = "https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.2/flash_attn-2.8.2+cu12torch2.7cxx11abiTRUE-cp312-cp312-linux_x86_64.whl" }
torch = [
    { index = "pytorch-cu126", marker = "sys_platform == 'linux' or sys_platform == 'win32'" },
]
torchaudio = [
    { index = "pytorch-cu126", marker = "sys_platform == 'linux' or sys_platform == 'win32'" },
]
torchvision = [
    { index = "pytorch-cu126", marker = "sys_platform == 'linux' or sys_platform == 'win32'" },
]
